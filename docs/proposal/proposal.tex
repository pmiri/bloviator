\documentclass[12pt]{article}

\usepackage{hyperref}

\title{
    COMP 498 Project Proposal: \\
    Automated Agents and Network Connectivity in Reddit Threads
 }
\author{
    Philippe H\'ebert \\
    Philippe Miriello \\
    Matthew Mongrain \\
    Frances Zsurka \\
}

\date{\today}

\begin{document}
    \maketitle
    We propose to study the relationship between the connectivity of a social network and its susceptibility to incursion by automated agents. To accomplish this, we will gather data from the social network Reddit and use them to construct language models that generate imitations of comment text, and examine to what extent our automated agents, using our language models, can ``pose'' as real members of a particular community. We will then evaluate how successful our agents have been using Reddit's system of votes, called \textit{karma}, and correlate the amount of karma gained to the strength of the network the agent operates in.

    This project comprises several components:

    \begin{enumerate}
        \item \textbf{Data collection.} First, we will select a set of individual message boards on Reddit, called \textit{subreddits}, and gather data from them. Reddit threads take the form of nested comment trees which descend from a single parent node, the \textit{post}. The data we collect, via Reddit's JSON API\footnote{\url{https://github.com/reddit/reddit/wiki/JSON}}, will consist of the content of individual comments, the comment's position within the comment tree, its author, and the amount of karma it received. To reduce the quantity of data we need to process as well as to obtain higher-quality (and more representative) data, we will focus our attention on posts that score above the average karma per post of the subreddit the post originates from.
        \item \textbf{Network modeling.} Using the threadedness of the comment data, we will attempt to construct a graph of the social network underlying each thread. We will say that two users are connected if there is a reply chain of length at least 3 involving both participants. In other words, if a user replies to a comment, and the person to whom they replied replies to their comment, we will say the two users are connected in the context of the thread.
        \item \textbf{Text generation.} Liberally inspired by Andrej Karpathy's \texttt{char-rnn}\footnote{\url{https://github.com/karpathy/char-rnn}} language model, we will train individual language models on the data obtained from each subreddit to allow us to post comments whose content is representative of each subredddit. One language model will be constructed for each subreddit we study. The model we will use is a type of recurrent neural network called a \textit{Long Short Term Memory network} (or LSTM). This model, which is trained at the character- rather than the word-level of a text, is capable of generating high-quality samples of text once trained. To improve the quality of our model, we will weight comments that received more karma more highly. We will then use Reddit's user API\footnote{\url{https://www.reddit.com/dev/api/}} to post comments generated by the model in an automated fashion and observe how much karma they receive from (presumably human) Reddit users.
        \item \textbf{Analysis.} Once we have gathered enough data from each subreddit, we will attempt to correlate certain factors---including the average amount of karma obtained by our automated agent per thread, the length and total karma of resultant comment chains, the sentiment of the thread, and the variety of commenters per thread---to the connectedness of that thread.
    \end{enumerate}
	\begin{thebibliography}{99}
		\bibitem{recurrentnetworkarch}
		Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever.
		\textit{An Empirical Exploration of Recurrent Network Architectures}.
		Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015.
		\bibitem{lstm}
		Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\'{i}k, Bas R. Steunebrink, and J\"{u}rgen Schmidhuber.
		\textit{LSTM: A Search Space Odyssey}.
		The Swiss AI Lab IDSIA, 2015.
		\bibitem{recurrentnets}
		Felix A. Gers and J\"{u}rgen Schmidhuber.
		\textit{Recurrent Nets that Time and Count}.
		The Swiss AI Lab IDSIA, Lugano, Switzerland.
		Douglas Guilbeault.
		\textit{Growing Bot Security: An Ecological View of Bot Agency}.
		University of Pennsylvania, USA, 2016.
		\bibitem{socialbots}
		Tim Hwang, Ian Pearce, and Max Nanis.
		\textit{Socialbots: Voices from the Fronts}.
		Social Mediator, pp. 38–45, 2012bitem{nltk}
        \bibitem{klein09}Steven Bird, Edward Loper and Ewan Klein,
        \emph{Natural Language Processing with Python},
        O'Reilly Media, Inc.,
        2009.
        \bibitem{karpathy}
            Andrej Karpathy,
            ``The Unreasonable Effectiveness of Recurrent Neural Networks'',
            \url{http://karpathy.github.io/2015/05/21/rnn-effectiveness},
            2015.
        \bibitem{goldberg}
            Yoav Goldberg,
            ``The Unreasonable Effectiveness of Character-level Language Models'',
            \url{http://nbviewer.jupyter.org/gist/joavg/d76121dfde2618422139},
            2015.
        \bibitem{brown92}
            Peter F. Brown,
            ``An Estimate of an Upper Bound for the Entropy of English'',
            Computational Linguistics 18.1,
            1992.
        \bibitem{gng}
        J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant et al., ``Quantitative analysis of culture using millions of digitized books,'' \textit{Science}, vol. 331, no. 6014, p. 176–182, 2011.

	\end{thebibliography}
\end{document}
